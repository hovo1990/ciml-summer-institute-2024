= How to fetch from upstream

[source,bash]
----
git remote add upstream git@github.com:ciml-org/ciml-summer-institute-2024.git
git fetch upstream   
git merge upstream/main
----


= Day 1 June 25, 2024


1. Parallel concepts (2.2.)
    1) Need good justification for ACCESS for CPU and GPU to convince
    2)     2.2_parallel_computing_concepts/Parallel concepts CIML.pdf
    3) **High throughput jobs** is when running many small jobs that use one core or one gpu
    4) Threads and processes are both independent sequences of execution
    5) Being aware of threads and processes will help you understand how your code is utilizing the hardware and identify common problems.
    6) MPI is for parallelizing C, C++ and Fortran code, is the defacto standard.
    7) Horovod and NCCL for parallelizing deep learning application.
    8) MPI pretty low level between assembly and C
    9) Use appropriate MPI wrappers
    10) MPI is library, OpenMP is an interface
    11) OpenMP more readable than MPI
    12)MPICH and OpenMPI are just implementation for MPI
    13) GCC includes and implements OpenMP standard
    14) 1 process per core on MPI
    15) Hybrid apps what balance, best way is to do benchmarking and see what works best
    16) Throwing more hardware it won't make faster and amdahl`s law is important
    17) Fraction means amount of work, not lines of code in amdal`s law
    18) The amount of serial will always limit your speed
    19) Another big problem is uneven load balancing
    20) Divide large problem into small chunks
    21) How do we know how many CPUs/GPUs to use. only way to do **scaling study**  for a **representative problem** is run on a different number of processors.
    22) Right way of scaling plot is **log axes**, code in https://github.com/hovo1990/Parallel-computing-concepts 
    23) close to 100% efficiency, nextflow also creates automatic checkpoints, jobs only running for 48 hours on the SDSC Expanse
2. Batch jobs with slurm
    1) HPC compute node dell c4140 in expanse with xeon gold 6248 and tesla v100
    2) ./2.3_getting_started_with_batch_jobs_scheduling/run-tf2-train-cnn-cifar.sh
    3) Interactive vs Batch computing
    4) Batch job is a pre-scripted sets of commands
    5) batch job scheduler is air traffic controller analogy
    6) Job-Shop problem
    7) all interactive jobs you run on an HPC system are usually treated the same as batch jobs
    8) SLURM one of the most popular schedulers
    9) People are "afraid" of command line
    10) Projects and accounts organized by Unix groups for accountability
    11) partition or queues the same thing, but different names on different schedulers
    12) sinfo command to query information about cluster
    13) squeue --me
    14) scancer <<job_id>>
    15) output hidden at .ood_portal when running tasks from the portal
    16) sbatch <<job_file.sh>>
    17) https://github.com/mkandes/galyleo?tab=readme-ov-file 
    18) galyleo output of the slurm job stored in .galyleo
3. Data filesystems and transfer
    1) wget, curl, aria2c 
    2) CIFAR-100 dataset for computer vision
    3) wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
    4) Bright cluster manager NVIDIA all the way on the EXPANSE
    5) module load aria2/1.35.0
    6) get info filesystem: cat /etc/auto.home | grep $USER
    7) don't download big data from github or gitlab (no git for big files)
    8) filesystem biggest problem for SDSC Expanse
    9) Lustre filesystem more finicky and fragile
    10) srun-shared is an alias
    11) scratch folder at /scratch/$USER/job_$SLUR_JOB_ID big update, much much faster
    12) Lustre behind the scenes quite complicated and can get fragile, designed for writing large files at an interval
    13) Ceph filesystem provides, block, file and object based storage,  ceph may be a more common target source and/or destination storage system for your data transfers in the future.
    14) Large systems not backed up, so copy your files back, and ceph a little bit more robust
    15)  What is GLOBUS endpoints and GLOBUS
    16) metadata operations are the most annoying one for lustre and ceph
    17) be mindful not to hurt the filesystem
    18) Globus (GridFTP) started by university of Chicago, parallel file transfer system
    19) Globus available from a web interface, it also checks data integrity and use parallel capabilities
    20) Globus are connected via research network with very fast internet connection
    21) They also have a command line tool
    22) https://github.com/globus/globus-compute 
