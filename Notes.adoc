= How to fetch from upstream

[source,bash]
----
git remote add upstream git@github.com:ciml-org/ciml-summer-institute-2024.git
git fetch upstream   
git merge upstream/main
----


= Day 1 June 25, 2024


1. Parallel concepts (2.2.)
    1) Need good justification for ACCESS for CPU and GPU to convince
    2)     2.2_parallel_computing_concepts/Parallel concepts CIML.pdf
    3) **High throughput jobs** is when running many small jobs that use one core or one gpu
    4) Threads and processes are both independent sequences of execution
    5) Being aware of threads and processes will help you understand how your code is utilizing the hardware and identify common problems.
    6) MPI is for parallelizing C, C++ and Fortran code, is the defacto standard.
    7) Horovod and NCCL for parallelizing deep learning application.
    8) MPI pretty low level between assembly and C
    9) Use appropriate MPI wrappers
    10) MPI is library, OpenMP is an interface
    11) OpenMP more readable than MPI
    12)MPICH and OpenMPI are just implementation for MPI
    13) GCC includes and implements OpenMP standard
    14) 1 process per core on MPI
    15) Hybrid apps what balance, best way is to do benchmarking and see what works best
    16) Throwing more hardware it won't make faster and amdahl`s law is important
    17) Fraction means amount of work, not lines of code in amdal`s law
    18) The amount of serial will always limit your speed
    19) Another big problem is uneven load balancing
    20) Divide large problem into small chunks
    21) How do we know how many CPUs/GPUs to use. only way to do **scaling study**  for a **representative problem** is run on a different number of processors.
    22) Right way of scaling plot is **log axes**, code in https://github.com/hovo1990/Parallel-computing-concepts 
    23) close to 100% efficiency, nextflow also creates automatic checkpoints, jobs only running for 48 hours on the SDSC Expanse
2. Batch jobs with slurm
    1) HPC compute node dell c4140 in expanse with xeon gold 6248 and tesla v100
    2) ./2.3_getting_started_with_batch_jobs_scheduling/run-tf2-train-cnn-cifar.sh
    3) Interactive vs Batch computing
    4) Batch job is a pre-scripted sets of commands
    5) batch job scheduler is air traffic controller analogy
    6) Job-Shop problem
    7) all interactive jobs you run on an HPC system are usually treated the same as batch jobs
    8) SLURM one of the most popular schedulers
    9) People are "afraid" of command line
    10) Projects and accounts organized by Unix groups for accountability
    11) partition or queues the same thing, but different names on different schedulers
    12) sinfo command to query information about cluster
    13) squeue --me
    14) scancer <<job_id>>
    15) output hidden at .ood_portal when running tasks from the portal
    16) sbatch <<job_file.sh>>
    17) https://github.com/mkandes/galyleo?tab=readme-ov-file  #VIP
    18) galyleo output of the slurm job stored in .galyleo
3. Data filesystems and transfer
    1) wget, curl, aria2c 
    2) CIFAR-100 dataset for computer vision
    3) wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
    4) Bright cluster manager NVIDIA all the way on the EXPANSE
    5) module load aria2/1.35.0
    6) get info filesystem: cat /etc/auto.home | grep $USER
    7) don't download big data from github or gitlab (no git for big files)
    8) filesystem biggest problem for SDSC Expanse
    9) Lustre filesystem more finicky and fragile
    10) srun-shared is an alias
    11) scratch folder at /scratch/$USER/job_$SLUR_JOB_ID big update, much much faster
    12) Lustre behind the scenes quite complicated and can get fragile, designed for writing large files at an interval
    13) Ceph filesystem provides, block, file and object based storage,  ceph may be a more common target source and/or destination storage system for your data transfers in the future.
    14) Large systems not backed up, so copy your files back, and ceph a little bit more robust
    15)  What is GLOBUS endpoints and GLOBUS
    16) metadata operations are the most annoying one for lustre and ceph
    17) be mindful not to hurt the filesystem
    18) Globus (GridFTP) started by university of Chicago, parallel file transfer system
    19) Globus available from a web interface, it also checks data integrity and use parallel capabilities
    20) Globus are connected via research network with very fast internet connection
    21) They also have a command line tool
    22) https://github.com/globus/globus-compute  #VIP
4. GPU computing
    1) for bigger parameters you need bigger data
    2) SDSC Voyager uses Intel Habana, some are for training, others for inference
    3) Newer generation liquid cooled, air cooled not enough for the supercomputer
    4) Harder to program on GPUs
    5)  less than 32 bit operations is worrisome
    6)  if you use ml framework such as pytorch no need to worry, but otherwise tricky
    7)  a millisecond simulation to do it right for MD
    8) Porting code to HIP and SYCL for Amber 
    9) Translating CUDA to HIP not so straightforward
    10) SYCL is nice, but NVIDIA not helping
    11) Move data only once at the beginning and use much as possible
    12) nvidia hpc is different from nvidia toolkit
    13) 
5. Containers
    1) Slurm batch examples in /cm/shared/examples/sdsc
    2) funny moment: Build one, run (almost) anywhere about containers
    3) multi node container jobs are sticky points
    4) docker not ideal in HPC
    5) docker not designed for batch based workflows
    6) https://github.com/mkandes/naked-singularity
    7) even building containers, run in interactive session, not on login node
    


[source,bash]
----
srun-shared
module load singularitypro

singularity version

#-- * not the usual one
singularity build docker://quay.io/jupyter/scipy-notebook:2024-05-27

#-- * the usual one

cd /scratch/$USER/job_$SLURM_JOB_ID
mkdir -p /scratch/$USER/job_$SLURM_JOB_ID/cache
export SINGULARITY_CACHEDIR="/scratch/$USER/job_$SLURM_JOB_ID/cache"
singularity pull docker://quay.io/jupyter/scipy-notebook:2024-05-27

singularity shell scipy-notebook_2024-05-27.sif 


singularity shell --bin /expanse,/scratch scipy-notebook_2024-05-27.sif 
----

    8) MPI versions must match to be able to use internode connection
    9) Put the singularity images on /expanse/lustre filesystem
    10) Nvidia has hpc-container-maker which builds singularity and docker images


= Day 2 


1. Machine Learning overview
    1) start jupyter-compute-tensorflow in 3.2 folder
    2) https://gist.github.com/willirath/2176a9fa792577b269cb393995f43dda intersesting how to deploy dask from container singularity and create slurm cluster #VIP
    3) a more interesting option on how to deploy dask https://gist.github.com/willirath/772e0de2b6fbe845f77388c3b16390ea better option #VIP
2. NN Raytune
    1) mpi is the typical parallezation stuff on HPC
    2) But, on HPC systems resources are shared so IP addresses are
dynamic
    3) mpirun -n number of tasks singularity -> python
    4) bigger batch size helps but uses more memory
    5) slurm and mpi work in tandem, ntasks-per-node means 8 instances
    6) 2 nodes 8 cores total 16 tasks so there are 16 ranks from 0-15
    7) mpi command runs the instances
    8) mapping genomic data onto structure https://github.com/sbl-sdsc/mmtf-genomics 
    9) https://www.ncbi.nlm.nih.gov/snp/ snp dataset #VIP
    10) https://www.ncbi.nlm.nih.gov/snp/ clinical data  #VIP
3. NN layer architecture
    1) vgg16-network simplest one for teaching
    2) autoencoder part of the architecture
    3) hidden layer named as "bottleneck" layer
    4) autoencoder for feature learning
    5) https://paperswithcode.com/sota #VIP interesting place
    6) https://github.com/danielmiessler/fabric/tree/main/patterns/analyze_patent #VIP interesting stuff
    7) lstm  architecture good for protein structure prediciton
    8) transformer architecture much better than the other ones, 
    9) transformer advantages over rnn, long range dependencies can be captures,
    10) bert is transformer variation
    11) bert is a language model, but not generative
    12) GPT is also a transformer, it is generative, it is pretrained on massive amounts of data, it is unsupervised, it just predicts the next word.
    13)  Transformer applications, nlp tasks, vision tasks, both
    14) GANs is generative, a generator - new samples, discriminator - determines if sample is generated or from input data(real)
    15) dataset of celeb faces
    16) GAN applications -> image to image translation
    17)  Generative AI ->  GAN, autoencoders, chatgpt example, dall-e, 
    18) codex is behined github copilot.
4. Transfer learning
    1)  ls -l data/catsVsDogs/train/cats/* | wc -l
    2) https://fugue-tutorials.readthedocs.io/tutorials/integrations/ecosystem/pycaret.html #VIP for the future
    3) Attention weight is the heart of the transformer
    4) ESM Fold by Meta uses language model for protein contacts
    5) molecule attention transformer, molecular property prediction
    6) https://github.com/ardigen/MAT github link for molecule attention transformer
    7) Bioclip for taxonomy


= Day 3

1. Conda environments on Expanse
    1) Peter Rose
    2) https://github.com/sbl-sdsc/df-parallel interesting #VIP #Examples
    3) do not create conda environment use the galyleo script!
    4) dask-cudf interesting
    5) Ask  for MultiNode Dask Cluster #VIP
    6) Run JupyterLab in batch example: https://github.com/sbl-sdsc/df-parallel/blob/main/problem.sh
    7) How to get account: apply allocations through ACCESS
    8) 1 GPU costs 20 CPU hours
    9) 1 millisecond it is accounted as 1 Service Unit
2. R
    1) R
    2) install.packages('doParallel') inside home folder
    3) Rscript --vanilla TestDoParallel_v1.R
    4) lasso regression used for variable selection
3. Spark
    1) exploits distributed memory to cache data, so in-memory processing
    2) Resilient Distributed Dataset (Collection of data)
    3) Data is divided into partitions
    4) Great mapReduce example
    5) Difference between DataFrame and DataSet 
    6) How to create dataframes
    7) Dask only Python, Spark more multi language support
    8) GraphFrames library for graph data #VIP #interesting
    9) for scaling only on the training set
    10) parallel plot for multi dimensional data
5. LLM
    1) Statistical relation between the words
    3) GPT-4 1.7 trillion parameter, but take with grain of salt, since it is closed source
    4) Common-crawl maintains a free, open repository for web crawl data https://commoncrawl.org/ #VIP #interesting
    5) Cost in US Dollars explosion
    6) Better measure of COST how many operations
    7) Mixture of experts (MoE) models
    8) great joke about context window
    9) think of as sliding window about tokens
    10) no prioritization of tokens, just a sliding window
    11) LLMs have an parameter named "temperature".
    12) LLMs are limited by training set
    13) LLM generated data will be used to train the next generation of LLMs
6. LLM prompt engineering
    1) good prompt engineering is important, clarity, specificity, and context
    2) For new topic create new chat, otherwise previous context will affect output
    3) make sure gemini and chatgpt have same output
    4) Marty Dask within conda much easier than inside a singularity container
