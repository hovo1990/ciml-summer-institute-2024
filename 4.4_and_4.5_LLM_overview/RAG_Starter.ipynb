{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa97a95-52c5-4e58-81f7-ed62c916ce27",
   "metadata": {},
   "source": [
    "# RAG â€“ Retrieval Augmented Generation\n",
    "\n",
    "## CIML Summer Institute\n",
    "\n",
    "#### UC San Diego\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d0f8c-b30a-4e71-9e0b-b1f431c5372b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this RAG tutorial, we'll be working with [LangChain](https://python.langchain.com/v0.2/docs/introduction/), which is a powerful framework for building applications with language models. LangChain provides utilities for working with various language model providers, integrating embeddings, and creating chains for more complex applications. Below are the necessary imports for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0922911-8123-41a2-85b9-f7b83e725776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ca64f-e1ee-4a9d-bf54-57e8c195d04a",
   "metadata": {},
   "source": [
    "We are also using [Ollama](https://ollama.com/), which is a platform for running LLMs on your local machine.  The following steps are needed to set up ollama for this RAG tutorial:\n",
    "1. In a terminal window in JupyterLab, type in the following command to start up the ollama service:\n",
    "**ollama serve**\n",
    "2. In another terminal window in JupyterLab, type in the following to download the model:  **ollama pull mistral**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c28c5e-392e-4ae0-8afe-dd663f32ce83",
   "metadata": {},
   "source": [
    "## Part 1: Retrieval\n",
    "\n",
    "- In this section, we'll focus on the retrieval aspect of RAG. We'll start by understanding vectorization, followed by storing and retrieving vectors efficiently.\n",
    "\n",
    "#### Vectorizing\n",
    "\n",
    "- **Vectorization** is the process of converting text into vectors in an embedding space. These vectors capture the semantic meaning of the text, enabling us to perform various operations like similarity calculations. We'll use HuggingFaceEmbeddings for this task, which is a class in LangChain that allows for an **embedding model** from HuggingFace to be integrated into LangChain applications. You can see the documentation for this LangChain class [here](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface.HuggingFaceEmbeddings.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763639e-a34c-4223-a11c-b838ae29f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258be401-ce04-43e0-9400-8a51bfb69a9f",
   "metadata": {},
   "source": [
    "This vectorizer converts text into vectors in embedding space. Lets try seeing how we can use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d355ea-1406-4da7-9778-f11efd6062e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.embed_query(\"dog\")[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ed0d1-412b-40dc-9a60-6ac60e88cf3d",
   "metadata": {},
   "source": [
    "- As you can see from above, this converts text into a series of numbers. \n",
    "\n",
    "### Task 1\n",
    "\n",
    "Your job is to **write a function that takes in two strings, vectorize them, and return their cosine similarity.** Implement the following function.\n",
    "\n",
    "#### `similarity_two_queries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957aa2c-911d-4a34-9e76-a396384a9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_two_queries(word1, word2):\n",
    "    # HINT:\n",
    "    # Use vectorizer.embed_query(<text>) to embed text.\n",
    "    # Use np.dot to find the cosine similarity/dot product of 2 vectors\n",
    "    # TODO\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0521f6f-630e-4e40-b66e-957c8e00f110",
   "metadata": {},
   "source": [
    "- Observe the similarity scores of both **'cat'** and **'dog'** to the word **'kitten'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c99caa-1b2f-4d53-92f3-140480908aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarity of 'kitten' and 'cat': \",similarity_two_queries(\"kitten\",\"cat\"))\n",
    "print(\"Similarity of 'kitten' and 'dog': \",similarity_two_queries(\"kitten\",\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121272b-fe4a-488c-89ed-320a6ae5dd24",
   "metadata": {},
   "source": [
    "- By using the previously defined function,  we can take pairs of texts and quantify how **similar** they are.\n",
    "\n",
    "### Task 2\n",
    "\n",
    "**Which of the following words in the list `words` are most related to the word 'color'?** The function `similarity_list` takes a list of words, and outputs the word and similarity score from highest to lowest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8100d-03b8-4b5e-beee-1959fc5250bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_list(word,list):\n",
    "    similarity_list = [(i,similarity_two_queries(\"color\",i)) for i in words]\n",
    "    sorted_similarity_list = sorted(similarity_list,key=lambda x:x[1],reverse=True)\n",
    "    return sorted_similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23161b1-5eb0-4646-a3e0-0d757873082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"rainbow\",\"car\",\"black\",\"red\",\"cat\",\"tree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d4d3d-e7d0-424e-a040-cb0481f19b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Which words are most similar to color?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408298c-e28c-4876-bc11-4d90510b9bf2",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Each query below has an appropriate text that allows you to answer the question. The function `match_queries_with_texts` matches a query with its most related text. **Come up with 3 more questions and 3 suitable answers and add them to the list below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b1718-3887-4923-8710-41f3ab0d64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_queries_with_texts(queries, texts):\n",
    "    # Calculate similarities between each query and text\n",
    "    similarities = np.zeros((len(queries), len(texts)))\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        for j, text in enumerate(texts):\n",
    "            similarities[i, j] = similarity_two_queries(query, text)\n",
    "    \n",
    "    # Match each query to the text with the highest similarity\n",
    "    matches = {}\n",
    "    for i, query in enumerate(queries):\n",
    "        best_match_idx = np.argmax(similarities[i])\n",
    "        matches[query] = texts[best_match_idx]\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62c3cf-808c-4368-b400-e63e95ccb3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the list to make suitable question-text pairs.\n",
    "\n",
    "queries = [\"What are the 7 colors of the rainbow?\", \n",
    "           \"What does Elsie do for work?\", \n",
    "           \"Which country has the largest population?\",\n",
    "           \"-- INSERT QUERY 1 HERE--\",\n",
    "           \"-- INSERT QUERY 2 HERE--\",\n",
    "           \"-- INSERT QUERY 3 HERE--\"]\n",
    "texts = [\"China has 1.4 billion people.\",\n",
    "         \"Elsie works the register at Arby's.\", \n",
    "         \"The colors of the rainbow are ROYGBIV.\",\n",
    "         \"-- INSERT TEXT 1 HERE--\",\n",
    "         \"-- INSERT TEXT 2 HERE--\",\n",
    "         \"-- INSERT TEXT 3 HERE--\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7f7c6-944b-4142-b3ec-546e9fc63920",
   "metadata": {},
   "source": [
    "- Now we shuffle the queries and texts. Let's see if we can match them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475dd2e-05e4-4ec8-9de4-006edfa606c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(queries)\n",
    "random.shuffle(texts)\n",
    "\n",
    "match_queries_with_texts(queries, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24525a35-9b13-46b3-9aa2-8dbe48714dfb",
   "metadata": {},
   "source": [
    "#### Database\n",
    "\n",
    "Now lets look at how we can store these for efficient retrieval of the vectors. There are many options for storage but in this exercise, we use [ChromaDB](https://python.langchain.com/v0.1/docs/integrations/vectorstores/chroma/)\n",
    " which is an open-source vector DB.\n",
    "\n",
    "Through langchain, we can set the database to be a LangChain ***retriever*** object, which essentially allows us to perform queries similarly to what we have done before.\n",
    "\n",
    "**Taking the `texts` and `queries` that you defined before, we can load it into ChromaDB and similarly perform the same operations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3bf18-a3c8-421a-9bf1-622372a250b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(range(len(texts)))\n",
    "db = Chroma.from_texts(texts, vectorizer, metadatas=[{\"id\": id} for id in ids])\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb640c-8719-4095-9bf5-ceeb7051a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f0dac-6bfd-4e3b-b6e3-e432122a2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"Which country has the largest population?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdfd53f-3620-4fa6-bc65-6d2466f76ca6",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "Now letâ€™s apply the same retrieval process to a file we read in. The file `workplaces.txt` contains names and workplaces of several people. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03765b-8e45-4db0-adcc-3b9eb2803f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"workplaces.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "lines = [line.strip() for line in lines]\n",
    "print(lines[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062cf178-d4c9-4910-845f-13f700a120e2",
   "metadata": {},
   "source": [
    "`workplace_retriever` is a function that takes in the workplace.txt file and returns a database as retriever that you can use to find out the workplaces of people in the file. You can specify the top-k results in the argument of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead1bb8-2f88-417d-9a22-4de7f9685354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def workplace_retriever(k=3):\n",
    "    with open(\"workplaces.txt\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    db = Chroma.from_texts(lines,vectorizer, metadatas=[{\"id\": id} for id in list(range(len(lines)))])\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679c17b-1943-4558-aec0-0a201c84a906",
   "metadata": {},
   "source": [
    "Using `workplace_retriever`, **find out who works at Starbucks and McDonald's**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4b386-f9fb-437f-8951-ad8400adcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find out who works at Starbucks and who works at McDonalds. Use the retriever(k=3).invoke(<query>) method to do this\n",
    "# You can experiment with the value of k to make sure you find all people that work in one place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bcccb-8dd3-4fc2-ba37-2c10ae4b409d",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "#### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6852173-e73d-4571-a1b8-6d3342a51117",
   "metadata": {},
   "source": [
    "The `workplaces.txt` data we just looked at was conveniently split into lines, with each line representing a distinct and meaningful chunk of information. This straightforward structure makes it easier to process and analyze the text data.\n",
    "\n",
    "However, it is usually not so straightforward:\n",
    "- When dealing with text data, especially from large or complex documents, it's essential to handle the formatting and structure efficiently.\n",
    "- If we get a not-so-simply formatted file, we can break it down into manageable chunks using LangChain's `TextLoader` and `RecursiveCharacterTextSplitter`.\n",
    "- This allows us to preprocess and chunk the data effectively for further use in our RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa4dd5-660b-4586-b4c0-934944f3233e",
   "metadata": {},
   "source": [
    "Lets take a look at some of the *Expanse* documentation [here](https://www.sdsc.edu/support/user_guides/expanse.html). We have downloaded the contents of this webpage into two text files named `expanse_doc_1.txt` and `expanse_doc_2.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90806506-2a56-4a27-bd74-ae0d44f29abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"expanse_doc_1.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "lines = [line.strip() for line in lines]\n",
    "print(lines[20:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9062ca2-37f0-4aee-95fd-9e70dbe8bd45",
   "metadata": {},
   "source": [
    "- We see that the data and text is not split into meaningful chunks of information by default, so we need to try out best to format it in such a way it can be useful. This is why we use chunks, which capture local and neighboring texts, grouping them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e0e3a-3093-4fcf-ae68-a2f92c3c3642",
   "metadata": {},
   "source": [
    "A function that chunks `expanse_doc_1.txt` has been provided below.  When using the RecursiveCharacterTextSplitter, the chunk size determines the maximum size of each text chunk. This is particularly useful when dealing with large documents that need to be split into smaller, manageable pieces for better retrieval and analysis.\n",
    "\n",
    "**Experiment with different chunk sizes** and pick a size that captures enough information to answer the question: ***\"How do you run jobs on expanse?\"*** Try sizes **10, 100 and 1000** and observe what info is being given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516bc647-7988-4aa5-9154-2e72af5367a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanse_retriever(chunk_size):\n",
    "    loader = TextLoader('expanse_doc_1.txt')\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=10, separators=[\" \", \",\", \"\\n\"])\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    db = Chroma(embedding_function=vectorizer)\n",
    "    db.add_documents(texts)\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c533f-4fa7-4f60-b7bb-8ea894404e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Think about how many characters would be needed to contain useful information for such a complex task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a48cb82-d6a2-4785-a68d-12437236633c",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "#### Multiple Document Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68743bad-9b2d-44c1-9cb0-1f4203869467",
   "metadata": {},
   "source": [
    "When we have more than one document we want to use in our database, we can simply iteratively chunk them. Metadata for the text source is added by default, but we can add our own metadata as well in the form of IDs.\n",
    "\n",
    "\n",
    "`expanse_all_retriever` is a function that chunks both `expanse_doc_1.txt` and `expanse_doc_2.txt` has been provided below, using a chunk size of 1000 characters, **Find which document information for \"Compiling Codes\" is most likely to be located.** *Hint: Look at the metadata*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746a3dd-e674-41c3-b0f8-22856530f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanse_all_retriever(chunk_size):\n",
    "    import glob\n",
    "    db = Chroma(embedding_function=vectorizer)\n",
    "    pattern = 'expanse_doc_*.txt'\n",
    "    file_list = glob.glob(pattern)\n",
    "    for file_name in file_list:\n",
    "        loader = TextLoader(file_name)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=10, separators=[\" \", \",\", \"\\n\"])\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        for id,text in enumerate(texts):\n",
    "            text.metadata[\"chunk_number\"] = id\n",
    "        db.add_documents(texts)\n",
    "\n",
    "    \n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642fd129-85b9-4909-92e4-212cee3345ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the relevant source for the query \"Compiling Codes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092f17f-faeb-4fee-8a00-3697a71988fa",
   "metadata": {},
   "source": [
    "## Part 2: Basic RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66182b0-291a-4913-803c-1f2894af1310",
   "metadata": {},
   "source": [
    "Ollama is an open-source LLM platform that allows us to use a plethora of different LLMs. In this notebook, Mistral is our LLM of choice. Feel free to play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e1036-dfc4-4ebf-abb8-b2da98b03e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = Ollama(model=\"mistral\")\n",
    "ollama.invoke(\"How are you doing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fae322-2770-45c2-a3d9-245f6ecaae55",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "**Write a function that uses the `workplace_retriever` function to parse your question, retrieves relevant responses from `workplace_retriever`, and then sends this context to Ollama for it to answer your question in natural language.** Fill in `workplace_question` which accomplishes this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fd5c4-32f0-40be-8823-90924dcfab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def workplace_question(question):\n",
    "    retriever = #TODO: assign the retriever\n",
    "    context = #TODO: invoke the retriever here\n",
    "    ollama = Ollama(model=\"mistral\")\n",
    "    prompt = f\"Based on the following context: {context}, answer the question: \"\n",
    "    response = #TODO: invoke ollama with the prompt and question\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a7fba-ef08-41cf-861b-255f76fcea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "workplace_question(\"Who works at Starbucks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0ca36-fcf4-449d-a21e-3e9310e39eaa",
   "metadata": {},
   "source": [
    "## Part 3: LangChain RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fda8a1-2152-4d38-bbee-775ba0d49110",
   "metadata": {},
   "source": [
    "The above is a very simple example of a RAG. Now, using langchain, we can put everything together in a cleaner and all inclusive way in one go. Let's combine everything we've learned into the function `generate_rag`.\n",
    "\n",
    "- The below implementation has a custom class that allows us to view what chunks are being used based on our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb5d18-0871-490b-b7d9-42c582918150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag(verbose=False, chunk_info=False):\n",
    "    import glob\n",
    "    vectorizer = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = Chroma(embedding_function=vectorizer)\n",
    "    pattern = 'expanse_doc_*.txt'\n",
    "    file_list = glob.glob(pattern)\n",
    "    for file_name in file_list:\n",
    "        loader = TextLoader(file_name)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10, separators=[\" \", \",\", \"\\n\"])\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        for id,text in enumerate(texts):\n",
    "            text.metadata[\"chunk_number\"] = id\n",
    "        db.add_documents(texts)\n",
    "    \n",
    "    template = \"\"\"<s>[INST] Given the context - {context} </s>[INST] [INST] Answer the following question - {question}[/INST]\"\"\"\n",
    "    pt = PromptTemplate(\n",
    "                template=template, input_variables=[\"context\", \"question\"]\n",
    "            )\n",
    "    # Let's retrieve the top 3 chunks for our results\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "    class CustomRetrievalQA(RetrievalQA):\n",
    "        def invoke(self, *args, **kwargs):\n",
    "            result = super().invoke(*args, **kwargs)\n",
    "            if chunk_info:\n",
    "                # Print out the chunks that were retrieved\n",
    "                print(\"Chunks being looked at:\")\n",
    "                chunks = retriever.invoke(*args, **kwargs)\n",
    "                for chunk in chunks:\n",
    "                    print(f\"Source: {chunk.metadata['source']}, Chunk number: {chunk.metadata['chunk_number']}\")\n",
    "                    print(f\"Text snippet: {chunk.page_content[:200]}...\\n\")  # Print the first 200 characters\n",
    "            return result\n",
    "    rag = CustomRetrievalQA.from_chain_type(\n",
    "        llm=Ollama(model=\"mistral\"),\n",
    "        retriever=retriever,\n",
    "        memory=ConversationSummaryMemory(llm=Ollama(model=\"mistral\")),\n",
    "        chain_type_kwargs={\"prompt\": pt, \"verbose\": verbose},\n",
    "    )\n",
    "\n",
    "    return rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f85b1e-4d5e-46b7-9fc4-82a993b2af2d",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "**Compare how mistral performs without context, and with context, i.e. without RAG and with RAG.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27716e1a-d3e7-4a3d-aa75-27a29919d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ollama.invoke(\"How do you check available resources on Expanse Supercomputer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe7824-f704-4dac-ba1a-55b872a66e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanse_rag = generate_rag()\n",
    "result = expanse_rag.invoke(\"How do you check available resources on Expanse Supercomputer\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44d0c4-a5fa-46c5-84a2-16ecc83a42c2",
   "metadata": {},
   "source": [
    "**We can see what is exactly being passed into the LLM highlighted in green when we set `verbose` to True.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba2b09-53b6-43d2-a29b-a5c260d87ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanse_rag = generate_rag(verbose=True)\n",
    "result = expanse_rag.invoke(\"How do you check available resources on Expanse Supercomputer\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bdc157-a204-4a4b-9970-92aa1b62e239",
   "metadata": {},
   "source": [
    "**For more concise information, the function defined allows us to see individual chunk details as well as their source.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7359d4a-2923-47c3-b0b3-b245681137be",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanse_rag = generate_rag(chunk_info=True)\n",
    "result = expanse_rag.invoke(\"How do you check available resources on Expanse Supercomputer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b21255-05d4-47d1-a1a7-5624f24e7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc228f4-d6b8-427d-b49b-78214fa606ec",
   "metadata": {},
   "source": [
    "#### Great work! We've officially made a chatbot that can help us out with all things *Expanse*, at least according to the 2 .txt files we have access to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f1d699-f672-4f64-8261-78cbcc58dd16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
