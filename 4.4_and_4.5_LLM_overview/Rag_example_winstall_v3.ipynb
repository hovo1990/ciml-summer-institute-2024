{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68783db-cfc8-40ba-9c3c-1f95d9eb8ed0",
   "metadata": {},
   "source": [
    "## A RAG example using Hugging Face documentation with LangChain\n",
    "\n",
    "This is a combination of tutorials from\n",
    "taken from https://huggingface.co/learn/cookbook/en/advanced_rag and\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/document_loaders/url/\n",
    "\n",
    "This notebook loads some text from url, splits into chunks, that make up the documents for RAG.  It then takes a user query, finds relevant documents, formats a prompt with context, and uses huggingface pipeline to get an answer from a Llama3 8B model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab002ea4-bbaa-45f3-8ce4-d6a455f70af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The tutorial would have your run the following\n",
    "#But it takes too long for us to wait, \n",
    "#  so we'll just use use pre-installed folders\n",
    "\n",
    "if 0:\n",
    "  !pip install --upgrade huggingface_hub[pytorch,cli] transformers accelerate datasets\n",
    "  !pip install --upgrade langchain sentence-transformers langchain-community\n",
    "  !pip install --upgrade bitsandbytes pypdf faiss-gpu pydantic\n",
    "  !pip install --upgrade langchain-huggingface\n",
    "  !pip install --upgrade unstructured\n",
    "  #now show all packages\n",
    "  !pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709b6aab-537f-4259-a865-9f16c5af3b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/train113/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/train113/.local/lib/python3.10/site-packages/transformers/utils/generic.py:481: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#from langchain.vectorstores import FAISS   #Facebook tool\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "#from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from typing import Optional, List, Tuple\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "print('imports done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0448603b-7f15-448a-b8b3-57f992127b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split doc funtion defined\n"
     ]
    }
   ],
   "source": [
    "#Functions to help split up document into chunks\n",
    "#  we'll use text from url in next cell\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: str, #EMBEDDING_MODEL_NAME\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "print('split doc funtion defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3deae16-2f0f-47a0-b3ec-b761e6eca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of raw pages after split: 5\n",
      "Length of docs: 30\n"
     ]
    }
   ],
   "source": [
    "#First set up loader and get web pages as the raw documents\n",
    "urls      = [ \"https://slurm.schedmd.com/quickstart.html\",\n",
    "               \"https://slurm.schedmd.com/man_index.html\"  ]\n",
    "\n",
    "loader    = UnstructuredURLLoader(urls=urls)\n",
    "raw_pages = loader.load_and_split()\n",
    "\n",
    "#raw_pages is a list\n",
    "print('Num of raw pages after split:',len(raw_pages))\n",
    "\n",
    "#Second set up a model to split the web pages\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "\n",
    "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "# This list is taken from LangChain's MarkdownTextSplitter class\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "    ]\n",
    "\n",
    "#Now split up the documents in to chunk size of tokens\n",
    "#  each chunk will be put in a database and \n",
    "#  each request will be 'keyword' matched to retrieve chunks\n",
    "#  that will be used as context for the prompt\n",
    "#  (but it's vectorized to be faster)\n",
    "docs_processed = split_documents(\n",
    "    256,        # chunk size <<<--- try diff size, too big is wasteful, too small useless\n",
    "    raw_pages,  \n",
    "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    ")\n",
    "\n",
    "print('Length of docs:', len(docs_processed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ea53de-eceb-4d35-820c-3ebd938937b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/train113/.local/lib/python3.10/site-packages/transformers/utils/generic.py:338: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/train113/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting document vector database retrieval for user_query='How to create a slurm job?'...\n",
      "=========== retrieved docs metadata  =============================\n",
      "{'source': 'https://slurm.schedmd.com/quickstart.html', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "#Third, set up embedding model and create vector database\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=False,  #True,   #this might cause some fork issues?\n",
    "    model_kwargs={\"device\": \"cpu\"},   # \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    "  )\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "  )\n",
    "\n",
    "#Now, embed a user query in the same space, show sample document\n",
    "user_query = \"How to create a slurm job?\"\n",
    "query_vector = embedding_model.embed_query(user_query)\n",
    "\n",
    "print(f\"\\nStarting document vector database retrieval for {user_query=}...\")\n",
    "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
    "print(\"=========== retrieved docs metadata  =============================\")\n",
    "print(retrieved_docs[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c170208-5138-4fa5-8366-d685989b5e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hugging face imports done\n"
     ]
    }
   ],
   "source": [
    "#Now setup the hugging face pipeline\n",
    "import huggingface_hub\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "print('hugging face imports done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5bcfcb9-4621-42c2-9843-2c859190660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to do this one time to save the auth token in \n",
    "#   ~/.cache/huggingface/token\n",
    "# Also, you might need to go to hugging face to get your auth token\n",
    "! ~/.local/bin/huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21f7401-0d74-401f-af69-90d550b22ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "#Set up model and tokenizer\n",
    "model=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "print('tokenizer loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e132c81a-270f-451e-9529-785526290fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG_PROMPT_TEMPLATE set up\n"
     ]
    }
   ],
   "source": [
    "  #Set up prompt template with a place for context informatoin\n",
    "  prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in the context,\n",
    "  give a comprehensive answer to the question.\n",
    "  Respond only to the question asked, response should be concise and relevant to the question.\n",
    "  Provide the number of the source document when relevant.\n",
    "  If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "  {context}\n",
    "  ---\n",
    "  Now here is the question you need to answer.\n",
    "\n",
    "  Question: {question}\"\"\",\n",
    "    },\n",
    "  ]\n",
    "  RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    "  )\n",
    "print('RAG_PROMPT_TEMPLATE set up')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cff9525-b914-43d4-b9c5-32e1377f7dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prompt beginning:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Using the information contained in the context,\n",
      "give a comprehensive answer to the questi   ....... \n",
      "Final prompt ending: \n",
      "ol for\n",
      "---\n",
      "Now here is the question you need to answer.\n",
      "\n",
      "Question: How to create a slurm job?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  #set up actual prompt with context consisting of retreived docs\n",
    "  retrieved_docs_text = [\n",
    "    doc.page_content for doc in retrieved_docs\n",
    "  ]  # We only need the text of the documents\n",
    "  context = \"\\nExtracted documents:\\n\"\n",
    "  context += \"\".join(\n",
    "    [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
    "  )\n",
    "\n",
    "  final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
    "    question=user_query, context=context\n",
    "  )\n",
    "\n",
    "#Final prompt is a long string \n",
    "print('Final prompt beginning:')\n",
    "print(final_prompt[0:150],'  ....... ')\n",
    "print('Final prompt ending: ')\n",
    "print(final_prompt[-150:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bcce10c-bc03-4c6a-89b7-c76d586d6c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.42s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline2 defined\n"
     ]
    }
   ],
   "source": [
    "  #set up the function \n",
    "  my_pipe2 = transformers.pipeline(\n",
    "    #\"text-generation\",\n",
    "    model=model,\n",
    "    #for gpu : \n",
    "    torch_dtype=torch.float16,\n",
    "    #torch_dtype=torch.float32,  #for cpu use this\n",
    "    device_map=\"auto\",\n",
    "    #device=device2use\n",
    "  )\n",
    "  print('pipeline2 defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d18e1c07-e2ae-4124-aace-96e050d198e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MYINFO mem allocated aft results: 16069058560\n",
      " ----------------- Generated Text Result --------------------------\n",
      "Result: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Using the information contained in the context,\n",
      "give a comprehensive answer to the question.\n",
      "Respond only to the question asked, response should be concise and relevant to the question.\n",
      "Provide the number of the source document when relevant.\n",
      "If the answer cannot be deduced from the context, do not give an answer.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context:\n",
      "\n",
      "Extracted documents:\n",
      "Document 0:::\n",
      "Slurm Workload Manager\n",
      "\n",
      "SchedMD\n",
      "\n",
      "Navigation\n",
      "\n",
      "Slurm Workload Manager\n",
      "\n",
      "Version 24.05\n",
      "\n",
      "About\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t\tOverview\n",
      "\t\t\t\t\t\tRelease Notes\n",
      "\n",
      "Using\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t\tDocumentation\n",
      "\t\t\t\t\t\tFAQ\n",
      "\t\t\t\t\t\tPublications\n",
      "\n",
      "Installing\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t\tDownload\n",
      "\t\t\t\t\t\tRelated Software\n",
      "\t\t\t\t\t\tInstallation Guide\n",
      "\n",
      "Getting Help\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t\tMailing Lists\n",
      "\t\t\t\t\t\tSupport and Training\n",
      "\t\t\t\t\t\tTroubleshooting\n",
      "\n",
      "Quick Start User Guide\n",
      "\n",
      "Overview\n",
      "\n",
      "Slurm is an open source,\n",
      "fault-tolerant, and highly scalable cluster management and job scheduling system\n",
      "for large and small Linux clusters. Slurm requires no kernel modifications for\n",
      "its operation and is relatively self-contained. As a cluster workload manager,\n",
      "Slurm has three key functions. First, it allocates exclusive and/or non-exclusive\n",
      "access to resources (compute nodes) to users for some duration of time so they\n",
      "can perform work. Second, it provides a framework for starting, executing, and\n",
      "monitoring work (normally a parallel job) on the set of allocated nodes. Finally,\n",
      "it arbitrates contention for resources by managing a queue of pending work.\n",
      "\n",
      "Architecture\n",
      "\n",
      "slurmd daemon running on\n",
      "each compute node and a centralDocument 1:::\n",
      "Figure 2. Slurm entities\n",
      "\n",
      "Commands\n",
      "\n",
      "Man pages exist for all Slurm daemons, commands, and API functions. The command\n",
      "option --help also provides a brief summary of\n",
      "options. Note that the command options are all case sensitive.\n",
      "\n",
      "sacct is used to report job or job\n",
      "step accounting information about active or completed jobs.\n",
      "\n",
      "salloc is used to allocate resources\n",
      "for a job in real time. Typically this is used to allocate resources and spawn a shell.\n",
      "The shell is then used to execute srun commands to launch parallel tasks.\n",
      "\n",
      "sattach is used to attach standard\n",
      "input, output, and error plus signal capabilities to a currently running\n",
      "job or job step. One can attach to and detach from jobs multiple times.\n",
      "\n",
      "sbatch is used to submit a job script\n",
      "for later execution. The script will typically contain one or more srun commands\n",
      "to launch parallel tasks.\n",
      "\n",
      "sbcast is used to transfer a file\n",
      "from local disk to local disk on the nodes allocated to a job. This can be\n",
      "used to effectively use diskless compute nodes or provide improved performance\n",
      "relative to a shared file system.Document 2:::\n",
      "Best Practices, Large Job Counts\n",
      "\n",
      "Consider putting related work into a single Slurm job with multiple job\n",
      "steps both for performance reasons and ease of management.\n",
      "Each Slurm job can contain a multitude of job steps and the overhead in\n",
      "Slurm for managing job steps is much lower than that of individual jobs.\n",
      "\n",
      "Job arrays are an efficient mechanism of\n",
      "managing a collection of batch jobs with identical resource requirements.\n",
      "Most Slurm commands can manage job arrays either as individual elements (tasks)\n",
      "or as a single entity (e.g. delete an entire job array in a single command).\n",
      "\n",
      "MPI\n",
      "\n",
      "MPI use depends upon the type of MPI being used.\n",
      "There are three fundamentally different modes of operation used\n",
      "by these various MPI implementations.\n",
      "\n",
      "Slurm directly launches the tasks and performs initialization of\n",
      "communications through the PMI2 or PMIx APIs. (Supported by most\n",
      "modern MPI implementations.)\n",
      "\n",
      "Slurm creates a resource allocation for the job and then\n",
      "mpirun launches tasks using Slurm's infrastructure (older versions of\n",
      "OpenMPI).Document 3:::\n",
      "Figure 1. Slurm components\n",
      "\n",
      "The entities managed by these Slurm daemons, shown in Figure 2, include\n",
      "nodes, the compute resource in Slurm,\n",
      "partitions, which group nodes into logical (possibly overlapping) sets,\n",
      "jobs, or allocations of resources assigned to a user for\n",
      "a specified amount of time, and\n",
      "job steps, which are sets of (possibly parallel) tasks within a job.\n",
      "The partitions can be considered job queues, each of which has an assortment of\n",
      "constraints such as job size limit, job time limit, users permitted to use it, etc.\n",
      "Priority-ordered jobs are allocated nodes within a partition until the resources\n",
      "(nodes, processors, memory, etc.) within that partition are exhausted. Once\n",
      "a job is assigned a set of nodes, the user is able to initiate parallel work in\n",
      "the form of job steps in any configuration within the allocation. For instance,\n",
      "a single job step may be started that utilizes all nodes allocated to the job,\n",
      "or several job steps may independently use a portion of the allocation.\n",
      "\n",
      "Figure 2. Slurm entities\n",
      "\n",
      "CommandsDocument 4:::\n",
      "information about Slurm nodes and partitions. slurm Slurm system overview. sprio View the factors that comprise a job's scheduling priority. squeue View information about jobs located in the Slurm scheduling queue. sreport Generate reports from the slurm accounting data. srun Run parallel jobs. sshare Tool for\n",
      "---\n",
      "Now here is the question you need to answer.\n",
      "\n",
      "Question: How to create a slurm job?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "To create a Slurm job, you can use the `sbatch` command, which submits a job script for later execution. The script will typically contain one or more `srun` commands to launch parallel tasks.\n"
     ]
    }
   ],
   "source": [
    "  #now call the function with the prompt as input and other options\n",
    "  results_list = my_pipe2(\n",
    "    final_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=5,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=500, #num new tokens to generate\n",
    "  )\n",
    "\n",
    "  mem_allocated = torch.cuda.memory_allocated()\n",
    "  print('MYINFO mem allocated aft results:', mem_allocated)\n",
    "\n",
    "  for result in results_list:   #result is a python dict object\n",
    "    print(' ----------------- Generated Text Result --------------------------')\n",
    "    print(f\"Result: {result['generated_text']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eddb49-66d9-4b2a-8982-6b0831e065d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
