{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2ad46e",
   "metadata": {},
   "source": [
    "# Iteratively Prompting GPT using OpenAI API\n",
    "\n",
    "In this tutorial, we'll explore how to harness the power of iterative prompting with OpenAI's GPT models. By feeding the chat history back into the model at each turn, we enable it to maintain context throughout the conversation, resulting in more coherent and relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8c18f",
   "metadata": {},
   "source": [
    "### Importing OpenAI API library\n",
    "\n",
    "This cell imports the OpenAI library into your Python environment. This is necessary to interact with OpenAI's API in your code. The openai library provides the functions and methods needed to send requests to OpenAI's models and receive their responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5592dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7160575a-76bc-43a7-81b7-c62975acd457",
   "metadata": {},
   "source": [
    "### Setting up API Key and Creating a Client\n",
    "\n",
    "This cell initializes the API key and model you will be using. The API key is a unique identifier that allows the OpenAI API to authenticate your requests. Make an API key using this [link](https://platform.openai.com/api-keys). It's essential to keep this key secure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5c9427-9c5d-45de-90a1-f1d9f2e18991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several ways to set your API key\n",
    "\n",
    "# Hard-code your API key\n",
    "# API_KEY = \"...\"      # Paste your API key here\n",
    "\n",
    "# Set an environment variable to hold your API key\n",
    "# API_KEY = os.getenv('API_KEY')\n",
    "\n",
    "# Get API_KEY from a file\n",
    "import json\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "API_KEY = config['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c065b96d-3824-44a3-bbc4-634c5e2a4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d159cbd-aaee-4019-bcd9-80567d9368bf",
   "metadata": {},
   "source": [
    "### Choosing a Model\n",
    "\n",
    "The model string specifies which version of the GPT model you intend to use for your requests. Look [here](https://platform.openai.com/docs/models/model-endpoint-compatibility) to check model endpoint compatibility. Make sure the API key and model are correct and valid for your use case. I have chosen a GPT-3.5 Turbo model for this tutorial as these models can understand and generate natural language and have been optimized for chat using the Chat Completions API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e6c88c-21a3-4d25-9062-d7dfb2359bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo-0125\"       # 16K context window, optimized for dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1394b79",
   "metadata": {},
   "source": [
    "### Defining a Function with Chat History Argument\n",
    "\n",
    "Here, we define a function `prompt_gpt` that interacts with OpenAI's chat model. It takes a list of messages as argument, prompts the user for input, appends the user's message to the list, sends the entire conversation history to GPT, retrieves the AI's response, appends it to the list, prints the AI's response, and returns the updated list of messages. If the user inputs \"stop\", it returns None to signal the end of the conversation. This function facilitates an iterative conversation loop with the GPT-3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ebec4d9-b4ef-4c65-aabb-7bcb58742053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_gpt(messages, system_message):\n",
    "    # input the user\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    # break condition\n",
    "    if user_input.lower() == 'stop':\n",
    "        return None  # return None to signal the end of the conversation\n",
    "    \n",
    "    # append user's dialog to messages\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # generate the response with history\n",
    "    response_with_history = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=100,\n",
    "        temperature=0.0,\n",
    "        seed=123\n",
    "    )\n",
    "    \n",
    "    # append GPT's dialog to messages (with history)\n",
    "    messages.append({\"role\": \"system\", \"content\": response_with_history.choices[0].message.content})\n",
    "    \n",
    "    # generate the response without history\n",
    "    response_without_history = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "        temperature=0.0,\n",
    "        seed=123\n",
    "    )\n",
    "    \n",
    "    # print the outputs\n",
    "    print(\"GPT (with history): \" + response_with_history.choices[0].message.content)\n",
    "    print(\"GPT (without history): \" + response_without_history.choices[0].message.content + \"\\n\")\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e399e35-db96-4d71-95e8-4db8aae02ed3",
   "metadata": {},
   "source": [
    "### Iteratively Prompting\n",
    "\n",
    "With the function defined, we are ready to iteratively prompt GPT. I initialize `messages` with a system role content. This does two things: (i) firstly, it helps constrain the entire conversation and allows you to specify the way the model answers questions, and (ii) secondly, as we continue to converse, the messages (chat history) will continue to grow. This will increase the API credit consumption and might exceed the input token limit. Initializing `messages` here ensures that each time we run the cell and start a conversation, the chat history refreshes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b964d6de-1867-4805-b11a-e569b6dc7565",
   "metadata": {},
   "source": [
    "### Tutorial\n",
    "\n",
    "In this tutorial, we'll demonstrate the importance of iteratively prompting a large language model (LLM) with conversation history to enhance the quality of its responses. As an example, we'll play a game of physician akinator with GPT. Here, the LLM asks us the most relevant questions to narrow down on what disease we have in mind. Try running the above code cell and successively prompting with the following instructions (feel free to select the right response based on the question asked):\n",
    "\n",
    "- \"I've been experiencing a persistent cough and a mild fever.\"\n",
    "- \"Yes, I have been in close contact with someone who is sick.\"\n",
    "- \"Yes, I have shortness of breath and fatigue.\"\n",
    "- \"Yes, I have a recent loss of taste and smell.\"\n",
    "\n",
    "Note that chat completions are non-deterministic by default.  Setting the `seed` parameter may help, but determinism is not guarantted.  For more information, see the [OpenAI API User Guide](https://platform.openai.com/docs/guides/text-generation/reproducible-outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "516cf5ca-6362-44e4-ad65-f0fdad1b0595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I've been experiencing a persistent cough and a mild fever.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (with history): Have you traveled recently or been in contact with anyone who is sick?\n",
      "GPT (without history): Have you traveled recently or been in contact with anyone who is sick?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Yes, I have been in close contact with someone who is sick.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (with history): Have you noticed any shortness of breath or chest pain along with the cough and fever?\n",
      "GPT (without history): Have you experienced any symptoms yourself since being in close contact with the sick individual?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Yes, I have shortness of breath and fatigue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (with history): Have you had any recent loss of taste or smell?\n",
      "GPT (without history): Do you have a cough or chest pain along with the shortness of breath and fatigue?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Yes, I have a recent loss of taste and smell.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (with history): COVID-19\n",
      "GPT (without history): Have you experienced any other symptoms such as fever, cough, or difficulty breathing?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  stop\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are a physician akinator - a highly skilled doctor trained at diagnosing a patient by repeatedly asking them relevant questions until you are completely sure of the disease. Once you are sure of the diagnosis, you respond with only the name of the disease as <disease>\"\n",
    "# system_message = \"You are a physician akinator - a highly skilled doctor trained at diagnosing a patient by repeatedly asking them relevant questions until you are completely sure of the disease. Once you are sure of the diagnosis, you respond with only the name of the disease.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message}\n",
    "]\n",
    "\n",
    "# enter \"stop\" to exit\n",
    "while True:\n",
    "    messages = prompt_gpt(messages, system_message)\n",
    "    if messages is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f53814-8609-49db-945d-ab5095288c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
